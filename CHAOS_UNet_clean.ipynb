{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d2c0d2",
   "metadata": {},
   "source": [
    "# CHAOS Liver Segmentation (CT) — U-Net (Clean Notebook)\n",
    "\n",
    "Ce notebook contient une version **propre** et **lisible** du pipeline de segmentation du foie sur le dataset **CHAOS (CT)**.\n",
    "\n",
    "**Résumé :**\n",
    "- Prétraitement : DICOM → HU → fenêtre *foie* fixe *(center=60, width=150)* → normalisation [0,1] → resize 256×256.\n",
    "- Masques : binaires (0/1), resize **NEAREST**.\n",
    "- Split : **par patient** (80/20).\n",
    "- Modèle : **U-Net** (Keras/TensorFlow 2.10), **loss = BCE + SoftDice**.\n",
    "- Évaluation : balayage de seuil (0.1–0.6) + **Largest Connected Component**.\n",
    "- Résultats val typiques : **Dice ≈ 0.71**, **IoU ≈ 0.68**.\n",
    "\n",
    "> ℹ️ L'appel d'entraînement `model.fit(...)` est **commenté** pour éviter un run accidentel (~6h). Décommentez si vous souhaitez réentraîner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ecac5",
   "metadata": {},
   "source": [
    "## 0) Préambule & chemins\n",
    "**Cellule (à écrire)** — imports, chemins, constantes, puis vérifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d02169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# dézipper\n",
    "import zipfile, os\n",
    "zip_name = 'CHAOS_Train_Sets.zip'\n",
    "if os.path.exists(zip_name):\n",
    "            zf.extractall('.') # extrait tous les fichiers du ZIP dans le dossier courant ('.').\n",
    "            print('Extrait:', zip_name)\n",
    "            \n",
    "CT_ROOT = Path('Train_Sets/CT')\n",
    "\n",
    "TARGET_SIZE = 256  # multiple de 16\n",
    "USE_WINDOW = True\n",
    "DEFAULT_CW = (60.0, 150.0)  # (center, width) CT abdomen\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42\n",
    "\n",
    "# ✅ Check attendu (à lancer quand variables prêtes)\n",
    "print(CT_ROOT.exists())  # → True\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d44e80",
   "metadata": {},
   "source": [
    "## 1) Lister les patients & en sélectionner quelques‑uns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef49d1",
   "metadata": {},
   "source": [
    "**Cellule — Lister**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = sorted(patients, key=lambda p: int(p.name))\n",
    "print([p.name for p in patients])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f49e7",
   "metadata": {},
   "source": [
    "**Cellule — Sélection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75985549",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PAT= 15 # choisi 15 patients sur 20\n",
    "import random; random.seed(SEED)\n",
    "sel_patients = sorted(random.sample(patients, N_PAT), key=lambda p: int(p.name))\n",
    "\n",
    "# ✅ Check attendu\n",
    "print([int(p.name) for p in sel_patients])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b263",
   "metadata": {},
   "source": [
    "## 2) Appairage **DICOM ↔ masque** par patient (filtrage masques vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e75de",
   "metadata": {},
   "source": [
    "**Cellule — : écrire `pairs_for_one_patient(p_dir)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_for_one_patient(p_dir, pos_to_neg_ratio=3, seed=1337): #     Crée les paires (DICOM, masque, id_patient). Garde toutes les tranches POSITIVES (avec foie). Sous-échantillonne les tranches NEGATIVES pour viser ~pos:neg ≈ 3:1.\n",
    "# Définit les chemins des deux sous-dossiers.\n",
    "    dcm_dir = p_dir / \"DICOM_anon\"\n",
    "    msk_dir = p_dir / \"Ground\"\n",
    "# Liste tous les fichiers .dcm et .png, puis trie (ordre alphabétique → correspondance par index).\n",
    "    dcms  = sorted(dcm_dir.glob(\"*.dcm\"))\n",
    "    masks = sorted(msk_dir.glob(\"*.png\"))\n",
    "    n = min(len(dcms), len(masks)) # prend min image entre les deux \n",
    "    \n",
    "    pos, neg = [], []\n",
    "    for i in range(n):\n",
    "        d_path, m_path = dcms[i], masks[i]\n",
    "        m = np.array(Image.open(m_path).convert(\"L\"))\n",
    "        if m.max() > 0:   # tranche POSITIVE (au moins un pixel foie)\n",
    "            pos.append((d_path, m_path, int(p_dir.name)))\n",
    "        else:             # tranche NEGATIVE\n",
    "            neg.append((d_path, m_path, int(p_dir.name)))\n",
    "\n",
    "    # Sous-échantillonnage des négatives pour ratio ≈ 3:1\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if len(pos) and len(neg):\n",
    "        max_neg = max(1, int(np.ceil(len(pos) / pos_to_neg_ratio)))\n",
    "        if len(neg) > max_neg:\n",
    "            neg = list(rng.choice(neg, size=max_neg, replace=False))\n",
    "\n",
    "    # Retourne toutes les positives + sous-échantillon de négatives\n",
    "    pairs = pos + neg\n",
    "    rng.shuffle(pairs)\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bedeea",
   "metadata": {},
   "source": [
    "**Cellule — Appliquer multi‑patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = []\n",
    "for pdir in sel_patients:\n",
    "    all_pairs.extend(pairs_for_one_patient(pdir, pos_to_neg_ratio=2))\n",
    "\n",
    "# ✅ Check attendu\n",
    "print('Total paires:', len(all_pairs))\n",
    "print(all_pairs[0])  # → (Path(...dcm), Path(...png), pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6eca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sum(np.array(Image.open(m).convert(\"L\")).max()>0 for _,m,_ in all_pairs)\n",
    "neg = len(all_pairs)-pos\n",
    "print(\"pos/neg =\", pos, \"/\", neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9877041",
   "metadata": {},
   "source": [
    "## 3) Split **par patient** (pas par slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fa14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pat_ids = sorted({pid for _,_,pid in all_pairs}) # for _,_,pid in all_pairs → on parcourt toutes les paires (DICOM, masque, patient_id). On ne garde que pid.\n",
    "n_train = int(0.8 * len(unique_pat_ids)) # 80% test\n",
    "train_ids = set(unique_pat_ids[:n_train]) # train = premier 80%\n",
    "val_ids   = set(unique_pat_ids[n_train:]) # validation= dernier 20%\n",
    "\n",
    "pairs_train = [(d,m) for (d,m,pid) in all_pairs if pid in train_ids] # Si l’ID patient est dans train_ids → on envoie la paire dans pairs_train.\n",
    "pairs_val   = [(d,m) for (d,m,pid) in all_pairs if pid in val_ids] # idem validation\n",
    "\n",
    "# ✅ Check attendu\n",
    "print('Patients train:', sorted(train_ids))\n",
    "print('Patients val  :', sorted(val_ids))\n",
    "print('Slices train/val:', len(pairs_train), len(pairs_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f40e7",
   "metadata": {},
   "source": [
    "## 4) Fonctions de **prétraitement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pydicom\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def apply_rescale(ds, arr):\n",
    "    slope = float(getattr(ds, 'RescaleSlope', 1.0))\n",
    "    intercept = float(getattr(ds, 'RescaleIntercept', 0.0))\n",
    "    return arr * slope + intercept\n",
    "\n",
    "def apply_window(ds, arr, default_cw=(60.0, 150.0)):\n",
    "    # CHANGEMENT: on n’utilise plus ds.WindowCenter/WindowWidth.\n",
    "    #             On force une fenêtre foie FIXE pour toutes les tranches/patients.\n",
    "\n",
    "    # CHANGEMENT: définir directement c,w = 60,150 (ou default_cw si tu veux garder un paramètre)\n",
    "    c, w = default_cw                     # CHANGEMENT: fenêtrage foie fixe\n",
    "\n",
    "    lo, hi = c - w/2.0, c + w/2.0         # bords de fenêtre\n",
    "    arr = np.clip(arr, lo, hi)            # clip dans [lo, hi]\n",
    "    arr = (arr - lo) / (hi - lo + 1e-6)   # map linéairement vers [0,1]\n",
    "    return arr\n",
    "\n",
    "def normalize01(arr):\n",
    "    # CHANGEMENT: on NE fait plus de min–max par tranche pour le CT.\n",
    "    # Si 'arr' sort de apply_window(...), il est déjà dans [0,1].\n",
    "    # On se contente d’assurer le dtype et de clipper au cas où.\n",
    "    arr = arr.astype(np.float32)\n",
    "    return np.clip(arr, 0.0, 1.0)   # CHANGEMENT: no-op stable (aucun recalage par image)\n",
    "\n",
    "def to_network_size(arr, target_size=256):\n",
    "    arr = cv2.resize(arr, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "    # CHANGEMENT: ne pas renormaliser; on reste dans [0,1] grâce à apply_window fixe\n",
    "    arr = np.clip(arr, 0.0, 1.0)        # sécurité légère\n",
    "    arr = arr[..., None].astype(\"float32\")\n",
    "    return arr\n",
    "\n",
    "def load_dicom_preprocessed(path, target_size=256, use_window=True):\n",
    "    ds  = pydicom.dcmread(str(path))                    # <-- utiliser 'path'\n",
    "    img = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "    # 1) Rescale (HU pour CT)\n",
    "    img = apply_rescale(ds, img)\n",
    "    \n",
    "    # 2) Fenêtrage FOIE FIXE -> [0,1]\n",
    "    # CHANGEMENT: même si use_window=False, on applique quand même la fenêtre fixe\n",
    "    # pour éviter TOUT min–max par tranche.\n",
    "    if use_window:\n",
    "        img = apply_window(ds, img)     # maintenant: (60,150) fixe\n",
    "    else:\n",
    "        img = apply_window(ds, img)     # CHANGEMENT: plus de normalize01 ici\n",
    "\n",
    "\n",
    "\n",
    "    # 3) Resize + (H,W,1) float32\n",
    "    img = to_network_size(img, target_size)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23317f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick un DICOM de ton dataset (adapter le chemin)\n",
    "some_dicom_path = \"Train_Sets/CT/1/DICOM_anon/i0007,0000b.dcm\"\n",
    "\n",
    "x = load_dicom_preprocessed(some_dicom_path)\n",
    "print(\"Shape:\", x.shape)        # attendu: (256,256,1)\n",
    "print(\"Dtype:\", x.dtype)        # attendu: float32\n",
    "print(\"Min/Max:\", float(x.min()), float(x.max()))  # attendu: ~0.0, ~1.0\n",
    "\n",
    "# Visual check\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[...,0], cmap='gray')\n",
    "plt.title(\"Window foie 60/150\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ed533",
   "metadata": {},
   "source": [
    "**Cellule — : `load_mask_binary(path)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7478bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = np.array(Image.open(path).convert('L'))\n",
    "# mask = (arr > 0).astype(np.uint8)\n",
    "# # resize NEAREST vers TARGET_SIZE\n",
    "# # retourner mask[..., None].astype('float32')\n",
    "import cv2\n",
    "import imageio.v2 as iio\n",
    "import numpy as np\n",
    "\n",
    "def load_mask_binary(path_png, target_size=256):\n",
    "    m = iio.imread(path_png)  # lit le PNG\n",
    "    if m.ndim == 3:           # si RGB → convertis en gris\n",
    "        m = cv2.cvtColor(m, cv2.COLOR_RGB2GRAY)\n",
    "    # binaire direct {0,1}\n",
    "    m = (m > 0).astype(np.uint8)\n",
    "    # resize en NEAREST\n",
    "    m = cv2.resize(m, (target_size, target_size), interpolation=cv2.INTER_NEAREST)\n",
    "    # (H,W,1) float32\n",
    "    m = m[..., None].astype(np.float32)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d38af",
   "metadata": {},
   "source": [
    "**Check attendu — vérif overlay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path, m_path, pid = all_pairs[3]\n",
    "img = load_dicom_preprocessed(d_path)\n",
    "msk = load_mask_binary(m_path)\n",
    "plt.figure(figsize=(9,3))\n",
    "plt.subplot(1,3,1); plt.imshow(img[...,0], cmap='gray'); plt.title('Image'); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(msk[...,0], cmap='gray'); plt.title('Mask GT'); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.imshow(img[...,0], cmap='gray'); plt.imshow(msk[...,0],cmap=\"Reds\", alpha=0.35); plt.title('Overlay'); plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2ec86",
   "metadata": {},
   "source": [
    "## 5) Construire `tf.data.Dataset` (train & val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe83c9",
   "metadata": {},
   "source": [
    "**Cellule — : listes de chemins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5380f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [str(d) for (d, m) in pairs_train]\n",
    "y_train = [str(m) for (d, m) in pairs_train]\n",
    "\n",
    "x_val   = [str(d) for (d, m) in pairs_val]\n",
    "y_val   = [str(m) for (d, m) in pairs_val]\n",
    "\n",
    "print(len(x_train), len(y_train), len(x_val), len(y_val))\n",
    "assert len(x_train) == len(y_train)\n",
    "assert len(x_val) == len(y_val)\n",
    "print(\"ex:\", x_train[0], \"->\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bafe48",
   "metadata": {},
   "source": [
    "**Cellule — : wrappers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08103c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  - décoder bytes → str\n",
    "#  - X = load_dicom_preprocessed(...); Y = load_mask_binary(...)\n",
    "#  - return X, Y\n",
    "\n",
    "# ========= 1) Loader Python (retourne des np.array) =========================\n",
    "def py_load_pair(path_dcm, path_png):\n",
    "    # tf.py_function passe des bytes -> on reconvertit en str Python\n",
    "    path_dcm = path_dcm.numpy().decode(\"utf-8\") # on extrait le chemin DICOM de chaque paire, et on le met en str (TF préfère str/bytes).\n",
    "    path_png = path_png.numpy().decode(\"utf-8\")\n",
    "    \n",
    "    X= load_dicom_preprocessed(path_dcm)\n",
    "    Y = load_mask_binary(path_png)\n",
    "\n",
    "    return X, Y  # np.ndarray (T,T,1), np.ndarray (T,T,1)\n",
    "\n",
    "\n",
    "\n",
    "#  - tf.py_function(py_load_pair, ..., Tout=[tf.float32, tf.float32])\n",
    "#  - X.set_shape([TARGET_SIZE, TARGET_SIZE, 1]); Y.set_shape(...)\n",
    "#  - return X, Y\n",
    "\n",
    "# ========= 2) Wrapper TensorFlow autour du loader Python ====================\n",
    "def tf_load_pair(path_dcm, path_png):\n",
    "    # Appelle py_load_pair côté Python depuis le graphe TF\n",
    "    X, Y = tf.py_function(\n",
    "        func=py_load_pair,                      # fonction Python à appeler\n",
    "        inp=[path_dcm, path_png],               # ses arguments (tensors -> bytes)\n",
    "        Tout=[tf.float32, tf.float32],          # types de sortie attendus\n",
    "    )\n",
    "    # IMPORTANT: fixer la shape statique pour que TF connaisse la taille\n",
    "    X.set_shape((TARGET_SIZE, TARGET_SIZE, 1))\n",
    "    Y.set_shape((TARGET_SIZE, TARGET_SIZE, 1))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# ========= 3) Data augmentation ====================\n",
    "def augment(img, mask):\n",
    "    # flip horizontal\n",
    "    if tf.random.uniform(()) > 0.5: # tf.random.uniform(()) génère un scalaire uniforme dans [0,1). > 0.5 ⇒ 50% de chances de rentrer dans le bloc.  On décide au hasard si on applique ou non le flip horizontal.\n",
    "    #On retourne l’image et le masque de gauche à droite. ⚠️ Très important : on applique exactement la même transfo aux deux, pour garder l’alignement.\n",
    "        img  = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "    # flip vertical\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        img  = tf.image.flip_up_down(img)\n",
    "        mask = tf.image.flip_up_down(mask)\n",
    "    # rotation 90°\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32) # Tire un entier k dans {0,1,2,3} : nombre de quarts de tour (0°, 90°, 180°, 270°).\n",
    "        img  = tf.image.rot90(img, k)\n",
    "        mask = tf.image.rot90(mask, k)\n",
    "    return img, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa7d90",
   "metadata": {},
   "source": [
    "**Cellule — : datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9210370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .map(tf_load_pair, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .map(augment,      num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .shuffle(512) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)) \\\n",
    "    .map(tf_load_pair, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ✅ Check attendu\n",
    "xb, yb = next(iter(ds_train))\n",
    "print(xb.shape, yb.shape, xb.dtype, yb.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd57664",
   "metadata": {},
   "source": [
    "## 7) Modèle U‑Net (léger) & compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1572f",
   "metadata": {},
   "source": [
    "**Cellule — : `build_unet(input_shape, base=32)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def enc(x,f):# encodeur\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    p = layers.MaxPooling2D()(x)\n",
    "    return x,p # Retourne s (skip = sortie avant pooling) et p (après pooling).\n",
    "\n",
    "def bottleneck(x,f): # Deux conv au fond du U (pas de pooling)\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    return x\n",
    "\n",
    "# - dec: Conv2DTranspose(..., strides=2) -> Concat skip -> Conv2D x2\n",
    "def dec(x,skip,f): # décodeur\n",
    "    x = layers.Conv2DTranspose(f,2,strides=2,padding=\"same\")(x)\n",
    "    x = layers.Concatenate()([x,skip])\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    x = layers.Conv2D(f,3,activation=\"relu\",padding=\"same\")(x)\n",
    "    return x\n",
    "# - sortie: Conv2D(1,1,activation='sigmoid')\n",
    "\n",
    "def build_unet(input_shape=(TARGET_SIZE,TARGET_SIZE,1)): # Construction du modèle complet\n",
    "    inp = layers.Input(input_shape)\n",
    "    s1,p1 = enc(inp,32);  s2,p2 = enc(p1,64)\n",
    "    s3,p3 = enc(p2,128);  s4,p4 = enc(p3,256)\n",
    "    b = bottleneck(p4,512)\n",
    "    d1 = dec(b,s4,256);  d2 = dec(d1,s3,128)\n",
    "    d3 = dec(d2,s2,64); d4 = dec(d3,s1,32)\n",
    "    out = layers.Conv2D(1,1,activation=\"sigmoid\")(d4)\n",
    "    return keras.Model(inp,out)\n",
    "\n",
    "\n",
    "model = build_unet() \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe4bff",
   "metadata": {},
   "source": [
    "**Cellule — compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  - simple : 'binary_crossentropy'\n",
    "#  - mieux (déséquilibre) : BCE + SoftDice (à ajouter plus tard)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1e-6):\n",
    "    # cast\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    # soft IoU (pas de seuillage ici)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1,2,3]) - intersection\n",
    "    return tf.reduce_mean((intersection + smooth) / (union + smooth))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    # cast\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    # soft Dice (pas de seuillage ici)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    denom = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    return tf.reduce_mean((2.0 * intersection + smooth) / (denom + smooth))\n",
    "\n",
    "bce = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    denom = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    # on retourne la moyenne batch\n",
    "    return 1.0 - tf.reduce_mean((2.0 * intersection + smooth) / (denom + smooth))\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return bce(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=bce_dice_loss,\n",
    "    metrics=[dice_coef, iou_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5999873",
   "metadata": {},
   "source": [
    "**Cellule — callbacks + fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2195dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Entraînement complet (~6h) : décommentez pour relancer\n",
    "\n",
    "#cb = [\n",
    "#    keras.callbacks.EarlyStopping(monitor=\"val_dice_coef\", mode=\"max\", patience=7, restore_best_weights=True),\n",
    "#    keras.callbacks.ModelCheckpoint(\"unet_dicom_best.h5\", monitor=\"val_dice_coef\", mode=\"max\", save_best_only=True),\n",
    "#    keras.callbacks.ReduceLROnPlateau(monitor=\"val_dice_coef\", mode=\"max\", factor=0.5, patience=3, min_lr=1e-5)\n",
    "#]\n",
    "# \n",
    "## history = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=[...])\n",
    "#hist = model.fit(ds_train, validation_data=ds_val, epochs=50, callbacks=cb) # dataset de validation, utilisé à la fin de chaque époque pour mesurer la performance.\n",
    "#\n",
    "#\n",
    "## ✅ Check attendu :\n",
    "#print(hist.history.keys())  # loss/val_loss, dice_coef/val_dice_coef, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e65de",
   "metadata": {},
   "source": [
    "## 8) Courbes d’entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e481023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option : ax.set_ylim(0,1) pour les métriques\n",
    "\n",
    "metrics = ['loss', 'dice_coef', 'iou_coef']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3,1, figsize=(8,12))  # 2 lignes × 2 colonnes\n",
    "axes = axes.ravel()  #on “aplatit” ce tableau pour avoir une liste [case1, case2, case3, case4] facile à parcourir dans une boucle.\n",
    "for i, m in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    ax.plot(hist.history[m], label=f'Train {m}')\n",
    "    ax.plot(hist.history[f'val_{m}'], label=f'Val {m}')\n",
    "    ax.set_title(m)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(m)\n",
    "    ax.set_ylim(0,1)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()  # pour éviter que les titres/légendes se chevauchent\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, cv2, matplotlib.pyplot as plt, tensorflow as tf\n",
    "\n",
    "# 1) Récupérer TOUTES les probas & GT du ds_val\n",
    "probs_list, gts_list, imgs_list = [], [], []\n",
    "for xb, yb in ds_val:\n",
    "    pb = model.predict(xb, verbose=0).squeeze(-1)   # (B,256,256), renvoie un tenseur (B, H, W, 1) de proba dans [0,1] et .squeeze(-1) supprime le dernier canal singleton\n",
    "    probs_list.append(pb)\n",
    "    gts_list.append(yb.numpy().squeeze(-1).astype(np.uint8))\n",
    "    imgs_list.append(xb.numpy())\n",
    "probs = np.concatenate(probs_list, 0)\n",
    "gts   = np.concatenate(gts_list,   0)\n",
    "imgs  = np.concatenate(imgs_list,  0)\n",
    "\n",
    "# 2) métriques binaires\n",
    "def dice_bin(a,b):\n",
    "    a = a.astype(bool); b = b.astype(bool)\n",
    "    inter = np.logical_and(a,b).sum()\n",
    "    return (2*inter)/(a.sum()+b.sum()+1e-7)\n",
    "def iou_bin(a,b):\n",
    "    a = a.astype(bool); b = b.astype(bool)\n",
    "    inter = np.logical_and(a,b).sum()\n",
    "    union = np.logical_or(a,b).sum()\n",
    "    return inter/(union+1e-7)\n",
    "\n",
    "# 3) plus grande composante\n",
    "def keep_largest_component(mask_bin_2d):\n",
    "    m = (mask_bin_2d.astype(np.uint8) > 0).astype(np.uint8)\n",
    "    if m.sum() == 0: return m\n",
    "    n, labels = cv2.connectedComponents(m, connectivity=8)# cv2.connectedComponents = OpenCV scanne l’image binaire et attribue un numéro différent à chaque “îlot” de pixels connectés.\n",
    "    if n <= 1: return m\n",
    "    sizes = [(labels==lab).sum() for lab in range(1, n)]\n",
    "    largest = int(np.argmax(sizes)) + 1\n",
    "    return (labels == largest).astype(np.uint8)\n",
    "\n",
    "# 4) chercher le meilleur seuil (0.1–0.6)\n",
    "ths = np.linspace(0.1, 0.6, 26)\n",
    "best_t, best_d = 0.5, -1.0\n",
    "for t in ths:\n",
    "    d = np.mean([dice_bin(g, (p>=t).astype(np.uint8)) for g,p in zip(gts, probs)])\n",
    "    if d > best_d: best_d, best_t = d, t\n",
    "print(f\"[val] meilleur seuil = {best_t:.2f} | Dice moyen (sans post) = {best_d:.4f}\")\n",
    "\n",
    "# 5) appliquer seuil + LCC et calculer Dice/IoU finaux\n",
    "preds_bin = []\n",
    "for p in probs:\n",
    "    b = (p >= best_t).astype(np.uint8)\n",
    "    b = keep_largest_component(b)\n",
    "    preds_bin.append(b)\n",
    "preds_bin = np.stack(preds_bin, 0)\n",
    "\n",
    "dice_final = np.mean([dice_bin(g, b) for g,b in zip(gts, preds_bin)])\n",
    "iou_final  = np.mean([iou_bin(g,  b) for g,b in zip(gts, preds_bin)])\n",
    "print(f\"[val] Dice (post) = {dice_final:.4f} | IoU (post) = {iou_final:.4f}\")\n",
    "\n",
    "# 6) afficher 3 exemples POSITIFS\n",
    "pos_idx = [i for i,g in enumerate(gts) if g.sum()>0]\n",
    "show = pos_idx[:3] if len(pos_idx)>=3 else list(range(min(3, len(gts))))\n",
    "n = len(show)\n",
    "plt.figure(figsize=(14, 4*n))\n",
    "for j,i in enumerate(show):\n",
    "    plt.subplot(n,4,4*j+1); plt.imshow(imgs[i,...,0], cmap='gray'); plt.title(\"Image\"); plt.axis('off')\n",
    "    plt.subplot(n,4,4*j+2); plt.imshow(gts[i], cmap='gray');         plt.title(\"Mask GT\"); plt.axis('off')\n",
    "    plt.subplot(n,4,4*j+3); plt.imshow(probs[i], cmap='gray', vmin=0, vmax=1); plt.title(\"Pred proba\"); plt.axis('off')\n",
    "    plt.subplot(n,4,4*j+4); plt.imshow(preds_bin[i], cmap='gray');   plt.title(f\"Pred @{best_t:.2f} + LCC\"); plt.axis('off')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc2336",
   "metadata": {},
   "source": [
    "## 9) Couverture (proportion de pixels positifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_coverage(ds):\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for _, y in ds.unbatch(): # ds_train.unbatch() → éclate chaque batch en exemples individuels (un (x,y) à la fois).\n",
    "        total += y.numpy().mean() # moyenne des pixels du masque (comme c’est binaire {0,1}, ça = % de pixels positifs).\n",
    "        count += 1\n",
    "    return total / count\n",
    "\n",
    "cov_train = compute_coverage(ds_train) # ~6.8% des pixels en moyenne appartiennent au foie.\n",
    "cov_val   = compute_coverage(ds_val) # ~5.5% des pixels en moyenne appartiennent au foie.\n",
    "print('Couverture Train :', cov_train)\n",
    "print('Couverture Val   :', cov_val)\n",
    "# Interprétation : ~0.00–0.02 → très rare (déséquilibre) ; ~0.05–0.15 → raisonnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc834b",
   "metadata": {},
   "source": [
    "## 11) Sauvegardes pour le README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea253890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Créer un dossier \"results\" si besoin\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# PARTIE 1 : Sauvegarder quelques figures (images val + GT + prédiction)\n",
    "# ============================================================\n",
    "for j,i in enumerate(show):  # \"show\" = indices déjà choisis d'images positives\n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    # Image CT\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(imgs[i,...,0], cmap='gray')\n",
    "    plt.title(\"Image\"); plt.axis('off')\n",
    "    \n",
    "    # Masque GT\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(gts[i], cmap='gray')\n",
    "    plt.title(\"Mask GT\"); plt.axis('off')\n",
    "    \n",
    "    # Overlay (image + préd)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(imgs[i,...,0], cmap='gray')\n",
    "    plt.imshow(preds_bin[i], alpha=0.4, cmap='Reds')  # alpha=0.4 pour transparence\n",
    "    plt.title(f\"Pred @{best_t:.2f} + LCC\"); plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/example_{j}.png\")  # Sauvegarde l’image\n",
    "    plt.close()  # Ferme la figure pour ne pas saturer la mémoire\n",
    "\n",
    "print(\"✅ Exemples sauvegardés dans results/\")\n",
    "\n",
    "# ============================================================\n",
    "# PARTIE 2 : Sauvegarder les courbes d'entraînement\n",
    "# ============================================================\n",
    "metrics = ['loss', 'dice_coef', 'iou_coef']\n",
    "fig, axes = plt.subplots(3,1, figsize=(8,12))\n",
    "axes = axes.ravel()\n",
    "for i, m in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    ax.plot(hist.history[m], label=f'Train {m}')\n",
    "    ax.plot(hist.history[f'val_{m}'], label=f'Val {m}')\n",
    "    ax.set_title(m); ax.set_xlabel('Epoch'); ax.set_ylabel(m)\n",
    "    ax.set_ylim(0,1); ax.grid(True); ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/training_curves.png\")\n",
    "plt.close()\n",
    "print(\"✅ Courbes d'entraînement sauvegardées dans results/training_curves.png\")\n",
    "\n",
    "# ============================================================\n",
    "# PARTIE 3 : Sauvegarder un résumé texte\n",
    "# ============================================================\n",
    "with open(\"results/summary.txt\", \"w\") as f:\n",
    "    f.write(\"=== Résumé expérimentation CHAOS Foie ===\\n\\n\")\n",
    "    f.write(\"Patients train : \" + str(sorted(train_ids)) + \"\\n\")\n",
    "    f.write(\"Patients val   : \" + str(sorted(val_ids)) + \"\\n\\n\")\n",
    "    f.write(f\"Couverture train : {cov_train:.3f}\\n\")\n",
    "    f.write(f\"Couverture val   : {cov_val:.3f}\\n\\n\")\n",
    "    f.write(f\"Meilleur seuil   : {best_t:.2f}\\n\")\n",
    "    f.write(f\"Dice final (val) : {dice_final:.4f}\\n\")\n",
    "    f.write(f\"IoU final (val)  : {iou_final:.4f}\\n\")\n",
    "print(\"✅ Résumé sauvegardé dans results/summary.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da857c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/summary.txt\", \"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bed20",
   "metadata": {},
   "source": [
    "---\n",
    "## 12) Notes\n",
    "- Le dataset **CHAOS** n'est pas inclus. Placez-le sous `Train_Sets/CT/<patient_id>/{DICOM_anon, Ground}`.\n",
    "- Les résultats (figures, résumé) sont sauvegardés dans `results/`.\n",
    "- Modèle entraîné : `unet_dicom_best.h5` (via `ModelCheckpoint`).\n",
    "- Pour reproduire l'évaluation, exécutez les cellules **post-entraînement** (balayage de seuil + LCC) sans relancer `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46785ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sarahchraibi/Projet_seg_GitHub'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae74a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAOS_Train_Sets.zip                \u001b[1m\u001b[36mresults\u001b[m\u001b[m\r\n",
      "CHAOS_UNet_clean.ipynb              \u001b[1m\u001b[36mTrain_Sets\u001b[m\u001b[m\r\n",
      "CHAOS_UNet_TODO_Guide_patched.ipynb unet_dicom_best.h5\r\n",
      "CHAOS_UNet_TODO_Guide.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f980e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
